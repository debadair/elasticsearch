[[getting-started]]
= Getting started with {es}

[partintro]
--
Ready to take {es} for a test drive and see for yourself how you can use the
REST APIs to store, search, and analyze data?

Follow this getting started tutorial to:

. Get an {es} cluster up and running
. Index some sample documents
. Search for documents using the {es} query language
. Analyze the results using bucket and metrics aggregations


Need more context?

Check out the <<elasticsearch-intro,
Elasticsearch Introduction>> to learn the lingo and understand the basics of
how {es} works. If you're already familiar with {es} and want to see how it works
with the rest of the stack, you might want to jump to the
{stack-gs}/get-started-elastic-stack.html[Elastic Stack
Tutorial] to see how to set up a system monitoring solution with {es}, {kib},
{beats},  and {ls}.

TIP: The fastest way to get started with {es} is to
https://www.elastic.co/cloud/elasticsearch-service/signup[start a free 14-day
trial of Elasticsearch Service] in the cloud.
--

[[getting-started-install]]
== Get {es} up and running

To take {es} for a test drive, you can create a one-click cloud deployment
on the https://www.elastic.co/cloud/elasticsearch-service/signup[Elasticsearch Service],
or <<run-elasticsearch-local, set up a basic {es} cluster>> on your own Linux,
macOS, or Windows machine.


[float]
[[run-elasticsearch-local]]
=== Run {es} locally on Linux, macOS, or Windows

When you create a cluster on the Elasticsearch Service, you automatically
get a three node cluster. To run a three node {es} cluster locally,
install {es} using the tar or zip archive:

. Download the Elasticsearch archive for your OS:
+
Linux: https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-{version}-linux-x86_64.tar.gz[elasticsearch-{version}-linux-x86_64.tar.gz]
+
["source","sh",subs="attributes,callouts"]
--------------------------------------------------
curl -L -O https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-{version}-linux-x86_64.tar.gz
--------------------------------------------------
// NOTCONSOLE
+
macOS: https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-{version}-darwin-x86_64.tar.gz[elasticsearch-{version}-darwin-x86_64.tar.gz]
+
["source","sh",subs="attributes,callouts"]
--------------------------------------------------
curl -L -O https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-{version}-darwin-x86_64.tar.gz
--------------------------------------------------
// NOTCONSOLE
+
Windows:
https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-{version}-windows-x86_64.zip[elasticsearch-{version}-windows-x86_64.zip]

. Extract the archive:
+
Linux:
+
["source","sh",subs="attributes,callouts"]
--------------------------------------------------
tar -xvf elasticsearch-{version}-linux-x86_64.tar.gz
--------------------------------------------------
+
macOS:
+
["source","sh",subs="attributes,callouts"]
--------------------------------------------------
tar -xvf elasticsearch-{version}-darwin-x86_64.tar.gz
--------------------------------------------------
+
Windows PowerShell:
+
["source","sh",subs="attributes,callouts"]
--------------------------------------------------
Expand-Archive elasticsearch-{version}-windows-x86_64.zip
--------------------------------------------------

. Start elasticsearch from the `bin` directory:
+
Linux and macOS:
+
["source","sh",subs="attributes,callouts"]
--------------------------------------------------
cd elasticsearch-{version}/bin
./elasticsearch
--------------------------------------------------
+
Windows:
+
["source","sh",subs="attributes,callouts"]
--------------------------------------------------
cd %PROGRAMFILES%\Elastic\Elasticsearch\bin
.\elasticsearch.exe
--------------------------------------------------
+
You now have a single-node {es} cluster up and running!

. Start two more instances of {es} so you can see how a typical multi-node
cluster behaves. You need to specify unique data and log paths
for each node.
+
Linux and macOS:
+
["source","sh",subs="attributes,callouts"]
--------------------------------------------------
./elasticsearch -Epath.data=data2 -Epath.logs=log2
./elasticsearch -Epath.data=data3 -Epath.logs=log3
--------------------------------------------------
+
Windows:
+
["source","sh",subs="attributes,callouts"]
--------------------------------------------------
.\elasticsearch.exe -Epath.data=data2 -Epath.logs=log2
.\elasticsearch.exe -Epath.data=data3 -Epath.logs=log3
--------------------------------------------------
+
The default configuration enables the new nodes to automatically join the
cluster with the first node because they are all running locally. Each node
is automatically assigned a unique ID.

. Use the `_cat/health` API to verify that your three node cluster is up running.
The `cat` APIs return information about your cluster and indices in a
format that's easier to read than raw JSON.
+
You can interact directly with your cluster by submitting HTTP requests to
the {es} REST API. Most of the examples in this guide enable you to copy the
appropriate cURL command and submit the request to your local {es} instance from
the command line. If you have Kibana installed and running, you can also
open Kibana and submit requests through the Dev Console.
+
TIP: When you're ready to start using {es} in your own applications, you'll
want to check out the
https://www.elastic.co/guide/en/elasticsearch/client/index.html[{es} language
clients].
+
[source,js]
--------------------------------------------------
GET /_cat/health?v
--------------------------------------------------
// CONSOLE
+
The response should indicate that the status of the _elasticsearch_ cluster
is _green_ and it has three nodes:
+
[source,txt]
--------------------------------------------------
epoch      timestamp cluster       status node.total node.data shards pri relo init unassign pending_tasks max_task_wait_time active_shards_percent
1565052807 00:53:27  elasticsearch green           3         3      6   3    0    0        0             0                  -                100.0%
--------------------------------------------------
// TESTRESPONSE[s/1565052807 00:53:27  elasticsearch/\\d+ \\d+:\\d+:\\d+ integTest/]
// TESTRESPONSE[s/3         3      6   3/\\d+         \\d+      \\d+   \\d+/]
// TESTRESPONSE[s/0             0                  -/0             \\d+                  -/]
// TESTRESPONSE[non_json]
+
NOTE: The cluster status will remain yellow if you are only running a single
instance of {es}. A single node cluster is fully functional, but data
cannot be replicated to another node to provide resiliency. Replica shards must
be available for the cluster status to be green. If the cluster status is red,
some data is unavailable.

[float]
[[gs-other-install]]
=== Other installation options

Installing {es} from an archive file enables you to easily install and run
multiple instances locally so you can try things out. To run a single instance,
you can  run {es} in a Docker container, install {es} using the DEB or RPM
packages on Linux, install using Homebrew on macOS, or install using the MSI
package installer on Windows. See <<install-elasticsearch>> for more information.

[[getting-started-index]]
== Index some data into {es}

Before you can start experimenting with queries and aggregations, you need
to add some data to an {es} index. {es} can ingest data that's formatted as JSON
objects, such as JSON documents shipped by Beats, Logstash, or your own
applications.

For this tutorial, you're going to set up a customer index. To index your first
document, submit the following POST request to your cluster:

[source,js]
--------------------------------------------------
POST /customer/_doc
{
  "name": "John Doe"
}
--------------------------------------------------
// CONSOLE

This request creates the _customer_ index, adds a document with the
specified data, and assigns it a unique documnent `_id`.

You can also assign your own doc _id when adding a document. For example:

[source,js]
--------------------------------------------------
PUT /customer/_doc/cid_1
{
  "name": "John Doe"
}
--------------------------------------------------
// CONSOLE

Note that {es} supports both HTTP PUT and POST requests for adding documents. If
you use PUT, you must explicitly specify the doc _id. To automatically generate
an _id, you must use POST.

When you index a document, {es} returns a response that shows the index, doc _id,
and version. For example:

[source,js]
--------------------------------------------------
{
  "_index" : "customer",
  "_type" : "_doc",
  "_id" : "cid_1",
  "_version" : 1,
  "result" : "created",
  "_shards" : {
    "total" : 2,
    "successful" : 1,
    "failed" : 0
  },
  "_seq_no" : 0,
  "_primary_term" : 1
}
--------------------------------------------------
// TESTRESPONSE[s/"_seq_no" : \d+/"_seq_no" : $body._seq_no/ s/"_primary_term" : 1/"_primary_term" : $body._primary_term/]

To update an existing document, you specify its doc _id:

[source,js]
--------------------------------------------------
PUT /customer/_doc/cid_1
{
  "name": "John Q. Doe"
}
--------------------------------------------------
// CONSOLE

Note that the response indicates that the document has been updated and the
version number is incremented:

[source,js]
--------------------------------------------------
{
  "_index" : "customer",
  "_type" : "_doc",
  "_id" : "cid_1",
  "_version" : 2,
  "result" : "updated",
  "_shards" : {
    "total" : 2,
    "successful" : 2,
    "failed" : 0
  },
  "_seq_no" : 11,
  "_primary_term" : 1
}
--------------------------------------------------
// TESTRESPONSE[s/"_seq_no" : \d+/"_seq_no" : $body._seq_no/ s/"_primary_term" : 1/"_primary_term" : $body._primary_term/]

When you add a document, it is immediately retrievable from any node in the
cluster. For example, you can retrieve the`cid_1` document you just indexed
from the node running on port 9201 with an HTTP GET request:

```http://localhost:9201/customer/_doc/cid_1```

This returns the document's JSON data in the `_source` field:

[source,js]
--------------------------------------------------
{
  "_index" : "customer",
  "_type" : "_doc",
  "_id" : "cid_1",
  "_version" : 2,
  "_seq_no" : 25,
  "_primary_term" : 1,
  "found" : true,
  "_source" : { "name": "John Q. Doe" }
}
--------------------------------------------------

[[getting-started-search]]
== Searching your data

The {es} query language enables you to use a combination of query and filter
criteria to search your data. You submit your query to the `_search`
API in the body of an HTTP GET or POST request, and {es} returns a list of
matching documents sorted by _relevance--how closely each document matches the
query criteria. Filter criteria control whether or not a document is included
in the search results, but have no impact on relevance scoring.

To get a feel for how this works, submit a simple `match_all` query against
the _customer_ index.

[source,js]
--------------------------------------------------
GET /customer/_search
{
  "query": { "match_all": {} }
}
--------------------------------------------------
// CONSOLE
// TEST

Like the name implies, this search matches every document in the index. The
response includes some statistics for the query and lists the first ten
matching documents in the `hits` array.

[source,js]
--------------------------------------------------
{
  "took" : 5,
  "timed_out" : false,
  "_shards" : {
    "total" : 1,
    "successful" : 1,
    "skipped" : 0,
    "failed" : 0
  },
  "hits" : {
    "total" : {
      "value" : 10,
      "relation" : "eq"
    },
    "max_score" : 1.0,
    "hits" : [
      {
        "_index" : "customer",
        "_type" : "_doc",
        "_id" : "2",
        "_score" : 1.0,
        "_source" : {
          "name" : "Jane Doe"
        }
      },
      {
        "_index" : "customer",
        "_type" : "_doc",
        "_id" : "LICsbmwBZcj6UWre1jDV",
        "_score" : 1.0,
        "_source" : {
          "name" : "Jane Adair"
        }
      },
      {
        "_index" : "customer",
        "_type" : "_doc",
        "_id" : "1",
        "_score" : 1.0,
        "_source" : {
          "name" : "John Doe"
        }
      },
      {
        "_index" : "customer",
        "_type" : "_doc",
        "_id" : "LYC2bmwBZcj6UWreczBF",
        "_score" : 1.0,
        "_source" : {
          "name" : "Jane Adair"
        }
      },
      {
        "_index" : "customer",
        "_type" : "_doc",
        "_id" : "LoC2bmwBZcj6UWrenTCE",
        "_score" : 1.0,
        "_source" : {
          "name" : "Jane Adair"
        }
      },
      {
        "_index" : "customer",
        "_type" : "_doc",
        "_id" : "L4C2bmwBZcj6UWresTDu",
        "_score" : 1.0,
        "_source" : {
          "name" : "Jane Adair"
        }
      },
      {
        "_index" : "customer",
        "_type" : "_doc",
        "_id" : "MIC2bmwBZcj6UWretTDg",
        "_score" : 1.0,
        "_source" : {
          "name" : "Jane Adair"
        }
      },
      {
        "_index" : "customer",
        "_type" : "_doc",
        "_id" : "MYC2bmwBZcj6UWreuTCb",
        "_score" : 1.0,
        "_source" : {
          "name" : "Jane Adair"
        }
      },
      {
        "_index" : "customer",
        "_type" : "_doc",
        "_id" : "cid_2",
        "_score" : 1.0,
        "_source" : {
          "name" : "John Doe"
        }
      },
      {
        "_index" : "customer",
        "_type" : "_doc",
        "_id" : "cid_1",
        "_score" : 1.0,
        "_source" : {
          "name" : "John Q. Doe"
        }
      }
    ]
  }
}
--------------------------------------------------
// TESTRESPONSE

[float]
=== Specifying multiple query criteria

Things are more interesting when you start combining multiple query clauses.
For example, instead of just getting a list of all of the customers with
`match_all`, you could find all of the customers with the last name _Doe_ who
are not named _John_.

The simplest way to construct this compound query is to use a `bool` query. You
can use a `must` clause to find the customers with the last name _Doe_, and
a _must_not_ clause to filter out anyone named _John_.

[source,js]
--------------------------------------------------
GET /customer/_search
{
  "query": {
    "bool": {
      "must" : {
        "term" : { "name" : "doe" }
      },
      "must_not" : {
        "term" : { "name" : "john" }
      }
    }
  }
}
--------------------------------------------------
// CONSOLE
// TEST



[float]
=== Adding filter criteria

Now that we have seen a few of the basic search parameters, let's dig in some more into the Query DSL. Let's first take a look at the returned document fields. By default, the full JSON document is returned as part of all searches. This is referred to as the source (`_source` field in the search hits). If we don't want the entire source document returned, we have the ability to request only a few fields from within source to be returned.

This example shows how to return two fields, `account_number` and `balance` (inside of `_source`), from the search:

[source,js]
--------------------------------------------------
GET /customer/_search
{
  "query": { "match_all": {} },
}
--------------------------------------------------
// CONSOLE
// TEST[continued]

Note that the above example simply reduces the `_source` field. It will still only return one field named `_source` but within it, only the fields `account_number` and `balance` are included.

If you come from a SQL background, the above is somewhat similar in concept to the `SQL SELECT FROM` field list.

Now let's move on to the query part. Previously, we've seen how the `match_all` query is used to match all documents. Let's now introduce a new query called the {ref}/query-dsl-match-query.html[`match` query], which can be thought of as a basic fielded search query (i.e. a search done against a specific field or set of fields).

This example returns the account numbered 20:

[source,js]
--------------------------------------------------
GET /bank/_search
{
  "query": { "match": { "account_number": 20 } }
}
--------------------------------------------------
// CONSOLE
// TEST[continued]

This example returns all accounts containing the term "mill" in the address:

[source,js]
--------------------------------------------------
GET /bank/_search
{
  "query": { "match": { "address": "mill" } }
}
--------------------------------------------------
// CONSOLE
// TEST[continued]

This example returns all accounts containing the term "mill" or "lane" in the address:

[source,js]
--------------------------------------------------
GET /bank/_search
{
  "query": { "match": { "address": "mill lane" } }
}
--------------------------------------------------
// CONSOLE
// TEST[continued]

This example is a variant of `match` (`match_phrase`) that returns all accounts containing the phrase "mill lane" in the address:

[source,js]
--------------------------------------------------
GET /bank/_search
{
  "query": { "match_phrase": { "address": "mill lane" } }
}
--------------------------------------------------
// CONSOLE
// TEST[continued]

Let's now introduce the {ref}/query-dsl-bool-query.html[`bool` query]. The `bool` query allows us to compose smaller queries into bigger queries using boolean logic.

This example composes two `match` queries and returns all accounts containing "mill" and "lane" in the address:

[source,js]
--------------------------------------------------
GET /bank/_search
{
  "query": {
    "bool": {
      "must": [
        { "match": { "address": "mill" } },
        { "match": { "address": "lane" } }
      ]
    }
  }
}
--------------------------------------------------
// CONSOLE
// TEST[continued]

In the above example, the `bool must` clause specifies all the queries that must be true for a document to be considered a match.

In contrast, this example composes two `match` queries and returns all accounts containing "mill" or "lane" in the address:

[source,js]
--------------------------------------------------
GET /bank/_search
{
  "query": {
    "bool": {
      "should": [
        { "match": { "address": "mill" } },
        { "match": { "address": "lane" } }
      ]
    }
  }
}
--------------------------------------------------
// CONSOLE
// TEST[continued]

In the above example, the `bool should` clause specifies a list of queries either of which must be true for a document to be considered a match.

This example composes two `match` queries and returns all accounts that contain neither "mill" nor "lane" in the address:

[source,js]
--------------------------------------------------
GET /bank/_search
{
  "query": {
    "bool": {
      "must_not": [
        { "match": { "address": "mill" } },
        { "match": { "address": "lane" } }
      ]
    }
  }
}
--------------------------------------------------
// CONSOLE
// TEST[continued]

In the above example, the `bool must_not` clause specifies a list of queries none of which must be true for a document to be considered a match.

We can combine `must`, `should`, and `must_not` clauses simultaneously inside a `bool` query. Furthermore, we can compose `bool` queries inside any of these `bool` clauses to mimic any complex multi-level boolean logic.

This example returns all accounts of anybody who is 40 years old but doesn't live in ID(aho):

[source,js]
--------------------------------------------------
GET /bank/_search
{
  "query": {
    "bool": {
      "must": [
        { "match": { "age": "40" } }
      ],
      "must_not": [
        { "match": { "state": "ID" } }
      ]
    }
  }
}
--------------------------------------------------
// CONSOLE
// TEST[continued]

[[getting-started-filters]]
=== Executing Filters

In the previous section, we skipped over a little detail called the document score (`_score` field in the search results). The score is a numeric value that is a relative measure of how well the document matches the search query that we specified. The higher the score, the more relevant the document is, the lower the score, the less relevant the document is.

But queries do not always need to produce scores, in particular when they are only used for "filtering" the document set. Elasticsearch detects these situations and automatically optimizes query execution in order not to compute useless scores.

The {ref}/query-dsl-bool-query.html[`bool` query] that we introduced in the previous section also supports `filter` clauses which allow us to use a query to restrict the documents that will be matched by other clauses, without changing how scores are computed. As an example, let's introduce the {ref}/query-dsl-range-query.html[`range` query], which allows us to filter documents by a range of values. This is generally used for numeric or date filtering.

This example uses a bool query to return all accounts with balances between 20000 and 30000, inclusive. In other words, we want to find accounts with a balance that is greater than or equal to 20000 and less than or equal to 30000.

[source,js]
--------------------------------------------------
GET /bank/_search
{
  "query": {
    "bool": {
      "must": { "match_all": {} },
      "filter": {
        "range": {
          "balance": {
            "gte": 20000,
            "lte": 30000
          }
        }
      }
    }
  }
}
--------------------------------------------------
// CONSOLE
// TEST[continued]

Dissecting the above, the bool query contains a `match_all` query (the query part) and a `range` query (the filter part). We can substitute any other queries into the query and the filter parts. In the above case, the range query makes perfect sense since documents falling into the range all match "equally", i.e., no document is more relevant than another.

In addition to the `match_all`, `match`, `bool`, and `range` queries, there are a lot of other query types that are available and we won't go into them here. Since we already have a basic understanding of how they work, it shouldn't be too difficult to apply this knowledge in learning and experimenting with the other query types.

[[getting-started-aggregations]]
=== Executing Aggregations

Aggregations provide the ability to group and extract statistics from your data. The easiest way to think about aggregations is by roughly equating it to the SQL GROUP BY and the SQL aggregate functions. In Elasticsearch, you have the ability to execute searches returning hits and at the same time return aggregated results separate from the hits all in one response. This is very powerful and efficient in the sense that you can run queries and multiple aggregations and get the results back of both (or either) operations in one shot avoiding network roundtrips using a concise and simplified API.

To start with, this example groups all the accounts by state, and then returns the top 10 (default) states sorted by count descending (also default):

[source,js]
--------------------------------------------------
GET /bank/_search
{
  "size": 0,
  "aggs": {
    "group_by_state": {
      "terms": {
        "field": "state.keyword"
      }
    }
  }
}
--------------------------------------------------
// CONSOLE
// TEST[continued]

In SQL, the above aggregation is similar in concept to:

[source,sh]
--------------------------------------------------
SELECT state, COUNT(*) FROM bank GROUP BY state ORDER BY COUNT(*) DESC LIMIT 10;
--------------------------------------------------

And the response (partially shown):

[source,js]
--------------------------------------------------
{
  "took": 29,
  "timed_out": false,
  "_shards": {
    "total": 5,
    "successful": 5,
    "skipped" : 0,
    "failed": 0
  },
  "hits" : {
     "total" : {
        "value": 1000,
        "relation": "eq"
     },
    "max_score" : null,
    "hits" : [ ]
  },
  "aggregations" : {
    "group_by_state" : {
      "doc_count_error_upper_bound": 20,
      "sum_other_doc_count": 770,
      "buckets" : [ {
        "key" : "ID",
        "doc_count" : 27
      }, {
        "key" : "TX",
        "doc_count" : 27
      }, {
        "key" : "AL",
        "doc_count" : 25
      }, {
        "key" : "MD",
        "doc_count" : 25
      }, {
        "key" : "TN",
        "doc_count" : 23
      }, {
        "key" : "MA",
        "doc_count" : 21
      }, {
        "key" : "NC",
        "doc_count" : 21
      }, {
        "key" : "ND",
        "doc_count" : 21
      }, {
        "key" : "ME",
        "doc_count" : 20
      }, {
        "key" : "MO",
        "doc_count" : 20
      } ]
    }
  }
}
--------------------------------------------------
// TESTRESPONSE[s/"took": 29/"took": $body.took/]

We can see that there are 27 accounts in `ID` (Idaho), followed by 27 accounts
in `TX` (Texas), followed by 25 accounts in `AL` (Alabama), and so forth.

Note that we set `size=0` to not show search hits because we only want to see the aggregation results in the response.

Building on the previous aggregation, this example calculates the average account balance by state (again only for the top 10 states sorted by count in descending order):

[source,js]
--------------------------------------------------
GET /bank/_search
{
  "size": 0,
  "aggs": {
    "group_by_state": {
      "terms": {
        "field": "state.keyword"
      },
      "aggs": {
        "average_balance": {
          "avg": {
            "field": "balance"
          }
        }
      }
    }
  }
}
--------------------------------------------------
// CONSOLE
// TEST[continued]

Notice how we nested the `average_balance` aggregation inside the `group_by_state` aggregation. This is a common pattern for all the aggregations. You can nest aggregations inside aggregations arbitrarily to extract pivoted summarizations that you require from your data.

Building on the previous aggregation, let's now sort on the average balance in descending order:

[source,js]
--------------------------------------------------
GET /bank/_search
{
  "size": 0,
  "aggs": {
    "group_by_state": {
      "terms": {
        "field": "state.keyword",
        "order": {
          "average_balance": "desc"
        }
      },
      "aggs": {
        "average_balance": {
          "avg": {
            "field": "balance"
          }
        }
      }
    }
  }
}
--------------------------------------------------
// CONSOLE
// TEST[continued]

This example demonstrates how we can group by age brackets (ages 20-29, 30-39, and 40-49), then by gender, and then finally get the average account balance, per age bracket, per gender:

[source,js]
--------------------------------------------------
GET /bank/_search
{
  "size": 0,
  "aggs": {
    "group_by_age": {
      "range": {
        "field": "age",
        "ranges": [
          {
            "from": 20,
            "to": 30
          },
          {
            "from": 30,
            "to": 40
          },
          {
            "from": 40,
            "to": 50
          }
        ]
      },
      "aggs": {
        "group_by_gender": {
          "terms": {
            "field": "gender.keyword"
          },
          "aggs": {
            "average_balance": {
              "avg": {
                "field": "balance"
              }
            }
          }
        }
      }
    }
  }
}
--------------------------------------------------
// CONSOLE
// TEST[continued]

There are many other aggregations capabilities that we won't go into detail here. The {ref}/search-aggregations.html[aggregations reference guide] is a great starting point if you want to do further experimentation.
